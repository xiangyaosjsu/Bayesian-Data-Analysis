# <p align="center">Large Language Model’s Performance in Solving Math Problems: Take ChatGPT 3.5 and Bard as Examples </p>
<p align="center"> Xiang Yao </p>
<p align="center"> December 2023 </p>

## <p align="center">Introduction </p>
<p align="justify"> This study aims to employ Bayesian analysis method to evaluate the accurate rate of ChatGPT 3.5 and Bard in solving math problems, and to discern which model outperforms another and how much better. Thirty randomly sampled math questions, from four sets of SAT math practical problems, are used to collected ChatGPT and Bard’s responses. Based on binomial likelihood and Beta Conjugate prior, an independent Beta posterior is derived respectively for each of the two Large Language models (LLM). Accordingly, a posterior PDF for the successful rate difference is obtained. 95% Highest Posterior Density (HPD) is computed to summarize posterior uncertainty. Posterior predictive distribution (PPD) and sensitivity analysis are utilized to conduct model checking. Monte Carlos simulation is implemented to replicate posterior distribution data and exhibit analysis results.  </p>
